---
title: "build interactions"
output: github_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
```

Our task today is to build interactions between each of the numeric variables. 

CHECK - `renaming` - Marlie already renamed the variables, so we don't need to do that step.

CHECK - `filtering` - CHECK - We also learned that we only want the institutions that are similar to BC, so those need filtering too. 

CHECK - `selecting` - There are a bunch of variables that we've kept for the sake of analysis (items like latitude and longitude). Those are not required for the sake of modeling. Before we build the interactions, we need to select only those variables we'll use. 

CHECK - `interaction` - After we do that, THEN we can build the interactions. Build the interactions based on the correlations Marlie posted to the google drive. Build interactions of anything with correlations greater than (absolute value .5)

CHECK - `splitting` - Then we should build training and test datasets. Hmmm, one more thing; since **Bellevue College** is our customer, we should make sure this isn't part of the training data somehow. 

Let's start by importing.

```{r message=FALSE}
library(readr)
institution_data <- read_csv(here::here("data/institution_data_explore_mc - institution_data.csv"))
```

## filtering
Then filtering for just bc-like colleges

```{r}
nrow(institution_data)

institution_data <- institution_data %>% 
  filter(level == "4-year",
         funding == "Public",
         flagship_binary == 0)

nrow(institution_data)
```

we've dropped the number of rows from 3,798 to 582; which is what we wanted.

## selecting 

Now we'll write some code to retain _only_ the modeling variables.

```{r}
bc_institution_data <- 
  institution_data %>% 
  select(institution_name,
         basic,
         grad_rate,
         student_count,
         spending_per_award,
         full_time_pct,
         full_time_count,
         med_sat_value,
         aid_value,
         endow_value,
         grad_on_time_pct, #ahh, i need to convert this to decimal as well.
         pell_value,
         fresh_retain_value,
         full_time_fac_pct) %>% 
  mutate(endow_value = as.integer(endow_value),
         med_sat_value = as.integer(med_sat_value),
         grad_on_time_pct = as.integer(grad_on_time_pct),
         grad_on_time_pct = grad_on_time_pct / 100)

head(bc_institution_data,5)
```

so we've dropped the number of columns from 23 to `r ncol(bc_institution_data)`; which is what we expect as well. What we DONT need are interactions with names or graduation rates. Names are just indentifiers and grad rates are what we're trying to predict.

## Filling NAs

After independent review from team members, we've decided to fill the NAs with median values. Consider this note simply documentation of the assumption.

```{r}
bc_institution_data$med_sat_value[is.na(bc_institution_data$med_sat_value)] <- 
  round(median(bc_institution_data$med_sat_value, na.rm = TRUE))

bc_institution_data$endow_value[is.na(bc_institution_data$endow_value)] <- 
  round(median(bc_institution_data$endow_value, na.rm = TRUE))

bc_institution_data$grad_on_time_pct[is.na(bc_institution_data$grad_on_time_pct)] <- 
  round(median(bc_institution_data$grad_on_time_pct, na.rm = TRUE))

bc_institution_data$fresh_retain_value[is.na(bc_institution_data$fresh_retain_value)] <- 
  round(median(bc_institution_data$fresh_retain_value, na.rm = TRUE))
```

## Interactions

Building interactions of the variables that showed high correlation in review.

```{r}
bc_institution_data <- bc_institution_data %>% 
  mutate(EndowXSpend = endow_value * spending_per_award,
         PellXSat = med_sat_value	*	pell_value,
         RetainXSat = fresh_retain_value	*	med_sat_value,
         AidXSat = 	aid_value * med_sat_value,
         AidXEndow = endow_value	*	aid_value)

glimpse(bc_institution_data)
summary(bc_institution_data)
```


```{r}
# just a quick exploration
bc_institution_data %>% 
  keep(is.numeric) %>% 
  gather(key = "key", value = "value") %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()
```

## splitting test and training data

```{r}
set.seed(310)
# borrowing code from: https://medium.com/@HollyEmblem/training-and-test-dataset-creation-with-dplyr-41d9aa7eab31 

# build an index column
bc_institution_data <- bc_institution_data %>% 
  mutate(id = row_number())

# make sure that worked
head(bc_institution_data$id)

# Create training set
train <- bc_institution_data %>% 
  sample_frac(.70)

# Create test set
test  <- anti_join(x = bc_institution_data, 
                   y = train, 
                   by = 'id')

# and double check that our 'customer' BC is in the test data
nrow(test)
test %>% 
  filter(institution_name == "Bellevue College")

# and triple check that our 'customer' isn't in the train data
nrow(train)
train %>% 
  filter(institution_name == "Bellevue College")
```

## Export data sets

```{r}
write_excel_csv(x = bc_institution_data,
                path = here::here("data/institution_data.csv"))

write_excel_csv(x = train,
                path = here::here("data/train_data.csv"))

write_excel_csv(x = test,
                path = here::here("data/test_data.csv"))
```



